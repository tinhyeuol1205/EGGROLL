"""
EGGROLL (Evolution Guided General Revolution Optimization via Low-rank Learning)
B∆∞·ªõc 1: Initialization - Chu·∫©n b·ªã m√¥ h√¨nh v√† hyperparameters
"""

import torch
import torch.nn as nn
from dataclasses import dataclass
from typing import Optional, Dict, Any
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    PreTrainedModel
)


@dataclass
class EGGROLLConfig:
    """
    C·∫•u h√¨nh hyperparameters cho EGGROLL. 
    
    Attributes:
        sigma: ƒê·ªô l·ªách chu·∫©n c·ªßa nhi·ªÖu (noise standard deviation)
        alpha: T·ªëc ƒë·ªô h·ªçc (learning rate)
        population_size: K√≠ch th∆∞·ªõc qu·∫ßn th·ªÉ N - s·ªë l∆∞·ª£ng bi·∫øn th·ªÉ m√¥ h√¨nh
        rank: H·∫°ng r c·ªßa ma tr·∫≠n nhi·ªÖu low-rank (r << d)
        use_antithetic: S·ª≠ d·ª•ng Antithetic Sampling ƒë·ªÉ gi·∫£m ph∆∞∆°ng sai
        target_modules: C√°c module s·∫Ω ƒë∆∞·ª£c finetune (None = t·∫•t c·∫£ linear layers)
        seed: Random seed ƒë·ªÉ t√°i t·∫°o k·∫øt qu·∫£
    """
    sigma: float = 0.01
    alpha: float = 1e-3
    population_size: int = 64
    rank: int = 16
    use_antithetic: bool = True
    target_modules: Optional[list] = None
    seed: Optional[int] = 42
    
    def __post_init__(self):
        """Validate hyperparameters."""
        assert self.sigma > 0, "sigma ph·∫£i > 0"
        assert self.alpha > 0, "alpha ph·∫£i > 0"
        assert self.population_size > 0, "population_size ph·∫£i > 0"
        assert self.rank > 0, "rank ph·∫£i > 0"
        
        if self.use_antithetic:
            # V·ªõi antithetic sampling, population_size ph·∫£i l√† s·ªë ch·∫µn
            assert self.population_size % 2 == 0, \
                "population_size ph·∫£i l√† s·ªë ch·∫µn khi d√πng antithetic sampling"


@dataclass
class ParameterInfo:
    """
    Th√¥ng tin v·ªÅ m·ªôt tham s·ªë c·∫ßn finetune.
    
    Attributes:
        name: T√™n c·ªßa parameter
        shape: K√≠ch th∆∞·ªõc c·ªßa parameter (d1, d2)
        dtype: Ki·ªÉu d·ªØ li·ªáu
        device: Device (cpu/cuda)
        original_param: Reference ƒë·∫øn parameter g·ªëc
    """
    name: str
    shape: tuple
    dtype: torch.dtype
    device: torch.device
    original_param: nn.Parameter


class EGGROLLInitializer:
    """
    Class kh·ªüi t·∫°o EGGROLL cho Translation Model.
    
    Qu·∫£n l√Ω vi·ªác:
    - Load pre-trained model
    - Freeze parameters
    - X√°c ƒë·ªãnh c√°c layers c·∫ßn finetune
    - Thi·∫øt l·∫≠p c·∫•u tr√∫c low-rank
    """
    
    def __init__(
        self,
        model_name_or_path: str,
        config: EGGROLLConfig,
        device: Optional[str] = None
    ):
        """
        Kh·ªüi t·∫°o EGGROLL.
        
        Args:
            model_name_or_path: T√™n ho·∫∑c ƒë∆∞·ªùng d·∫´n ƒë·∫øn pre-trained model
            config: C·∫•u h√¨nh EGGROLL
            device: Device ƒë·ªÉ load model ('cuda', 'cpu', ho·∫∑c None ƒë·ªÉ t·ª± ƒë·ªông)
        """
        self.config = config
        self.device = device or ('cuda' if torch. cuda.is_available() else 'cpu')
        
        # Set random seed cho reproducibility
        if config.seed is not None:
            torch.manual_seed(config.seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(config.seed)
        
        # Load model v√† tokenizer
        print(f"[B∆∞·ªõc 1. 1] Loading pre-trained model: {model_name_or_path}")
        self.model = self._load_model(model_name_or_path)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
        
        # Freeze t·∫•t c·∫£ parameters
        print("[B∆∞·ªõc 1.2] Freezing all model parameters...")
        self._freeze_model()
        
        # X√°c ƒë·ªãnh c√°c parameters s·∫Ω ƒë∆∞·ª£c perturb
        print("[B∆∞·ªõc 1.3] Identifying target parameters for low-rank perturbation...")
        self. target_params = self._identify_target_parameters()
        
        # T√≠nh to√°n v√† hi·ªÉn th·ªã th·ªëng k√™
        self._print_statistics()
    
    def _load_model(self, model_name_or_path: str) -> PreTrainedModel:
        """Load pre-trained translation model."""
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name_or_path,
            torch_dtype=torch.float32,  # C√≥ th·ªÉ ƒë·ªïi sang float16 ƒë·ªÉ ti·∫øt ki·ªám memory
        )
        model = model.to(self.device)
        model.eval()  # Set evaluation mode
        return model
    
    def _freeze_model(self):
        """Freeze t·∫•t c·∫£ parameters - ES kh√¥ng c·∫ßn gradient."""
        for param in self.model. parameters():
            param.requires_grad = False
    
    def _identify_target_parameters(self) -> Dict[str, ParameterInfo]:
        """
        X√°c ƒë·ªãnh c√°c parameters s·∫Ω ƒë∆∞·ª£c perturb v·ªõi low-rank matrices.
        
        M·∫∑c ƒë·ªãnh: T·∫•t c·∫£ Linear layers (nn.Linear) trong Transformer. 
        C√≥ th·ªÉ customize qua config.target_modules. 
        """
        target_params = {}
        
        for name, module in self.model. named_modules():
            # Ch·ªâ x√©t Linear layers (ho·∫∑c c√°c modules ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh)
            if isinstance(module, nn. Linear):
                # Ki·ªÉm tra n·∫øu c√≥ target_modules filter
                if self. config.target_modules is not None:
                    if not any(target in name for target in self.config.target_modules):
                        continue
                
                # L∆∞u th√¥ng tin parameter weight
                param = module.weight
                param_info = ParameterInfo(
                    name=f"{name}.weight",
                    shape=param.shape,
                    dtype=param.dtype,
                    device=param. device,
                    original_param=param
                )
                target_params[f"{name}.weight"] = param_info
        
        return target_params
    
    def _print_statistics(self):
        """In th·ªëng k√™ v·ªÅ model v√† EGGROLL configuration."""
        # T·ªïng s·ªë parameters c·ªßa model
        total_params = sum(p.numel() for p in self.model.parameters())
        
        # S·ªë parameters s·∫Ω ƒë∆∞·ª£c perturb
        target_params_count = sum(
            info.shape[0] * info.shape[1] 
            for info in self.target_params. values()
        )
        
        # Memory ti·∫øt ki·ªám ƒë∆∞·ª£c v·ªõi low-rank
        # Full perturbation: N * target_params_count * 4 bytes (float32)
        # Low-rank: N * sum((d1 + d2) * r) * 4 bytes
        full_memory = self.config.population_size * target_params_count * 4
        lowrank_memory = self.config.population_size * sum(
            (info.shape[0] + info.shape[1]) * self. config.rank * 4
            for info in self.target_params. values()
        )
        memory_saved_ratio = (1 - lowrank_memory / full_memory) * 100
        
        print("\n" + "="*60)
        print("EGGROLL INITIALIZATION COMPLETE")
        print("="*60)
        print(f"\nüìä Model Statistics:")
        print(f"   - Total parameters: {total_params:,}")
        print(f"   - Target parameters (for perturbation): {target_params_count:,}")
        print(f"   - Number of target layers: {len(self.target_params)}")
        
        print(f"\n‚öôÔ∏è  EGGROLL Hyperparameters:")
        print(f"   - œÉ (noise std): {self.config.sigma}")
        print(f"   - Œ± (learning rate): {self.config.alpha}")
        print(f"   - N (population size): {self.config.population_size}")
        print(f"   - r (rank): {self.config. rank}")
        print(f"   - Antithetic sampling: {self.config.use_antithetic}")
        
        print(f"\nüíæ Memory Efficiency:")
        print(f"   - Full perturbation memory: {full_memory / 1e9:.2f} GB")
        print(f"   - Low-rank perturbation memory: {lowrank_memory / 1e9:. 4f} GB")
        print(f"   - Memory saved: {memory_saved_ratio:.2f}%")
        print("="*60 + "\n")
    
    def get_parameter_shapes(self) -> Dict[str, tuple]:
        """Tr·∫£ v·ªÅ dictionary mapping t√™n parameter -> shape."""
        return {name: info.shape for name, info in self.target_params.items()}
    
    def get_original_parameters(self) -> Dict[str, torch.Tensor]:
        """
        Tr·∫£ v·ªÅ b·∫£n sao c·ªßa parameters g·ªëc Œ∏.
        D√πng l√†m baseline cho perturbation.
        """
        return {
            name: info.original_param. data.clone()
            for name, info in self.target_params.items()
        }


def initialize_eggroll(
    model_name: str = "Helsinki-NLP/opus-mt-en-vi",
    sigma: float = 0.01,
    alpha: float = 1e-3,
    population_size: int = 64,
    rank: int = 16,
    use_antithetic: bool = True,
    target_modules: Optional[list] = None,
    seed: int = 42
) -> EGGROLLInitializer:
    """
    Convenience function ƒë·ªÉ kh·ªüi t·∫°o EGGROLL cho Translation Model.
    
    Args:
        model_name: T√™n pre-trained model (v√≠ d·ª•: Helsinki-NLP/opus-mt-en-vi)
        sigma: ƒê·ªô l·ªách chu·∫©n c·ªßa nhi·ªÖu
        alpha: Learning rate
        population_size: K√≠ch th∆∞·ªõc qu·∫ßn th·ªÉ
        rank: H·∫°ng c·ªßa low-rank matrices
        use_antithetic: S·ª≠ d·ª•ng antithetic sampling
        target_modules: List c√°c module names ƒë·ªÉ filter (None = all linear layers)
        seed: Random seed
    
    Returns:
        EGGROLLInitializer instance ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
    
    Example:
        >>> eggroll = initialize_eggroll(
        ...     model_name="Helsinki-NLP/opus-mt-en-vi",
        ...      sigma=0. 01,
        ...      alpha=1e-3,
        ...     population_size=64,
        ...     rank=16
        ... )
    """
    config = EGGROLLConfig(
        sigma=sigma,
        alpha=alpha,
        population_size=population_size,
        rank=rank,
        use_antithetic=use_antithetic,
        target_modules=target_modules,
        seed=seed
    )
    
    return EGGROLLInitializer(
        model_name_or_path=model_name,
        config=config
    )


# ============================================================
# EXAMPLE USAGE
# ============================================================
if __name__ == "__main__":
    # V√≠ d·ª• 1: Kh·ªüi t·∫°o v·ªõi model d·ªãch Anh-Vi·ªát
    print("üöÄ Initializing EGGROLL for English-Vietnamese Translation Model\n")
    
    eggroll = initialize_eggroll(
        model_name="Helsinki-NLP/opus-mt-en-vi",  # Model d·ªãch Anh -> Vi·ªát
        sigma=0.01,           # Noise standard deviation
        alpha=1e-3,           # Learning rate
        population_size=64,   # S·ªë candidates m·ªói iteration
        rank=16,              # Low-rank dimension
        use_antithetic=True,  # Gi·∫£m variance
        seed=42
    )
    
    # Truy c·∫≠p c√°c th√†nh ph·∫ßn
    print("üìù Accessible components:")
    print(f"   - Model: {type(eggroll. model).__name__}")
    print(f"   - Tokenizer: {type(eggroll.tokenizer).__name__}")
    print(f"   - Config: {eggroll.config}")
    print(f"   - Number of target params: {len(eggroll.target_params)}")
    
    # L·∫•y shapes ƒë·ªÉ chu·∫©n b·ªã cho B∆∞·ªõc 2 (t·∫°o low-rank matrices)
    param_shapes = eggroll.get_parameter_shapes()
    print(f"\nüìê First 5 parameter shapes (for low-rank matrix generation):")
    for i, (name, shape) in enumerate(list(param_shapes. items())[:5]):
        print(f"   {name}: {shape} -> A: ({shape[0]}, {eggroll.config. rank}), B: ({shape[1]}, {eggroll.config. rank})")
