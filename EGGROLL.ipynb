{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa990TdSBs8QWDRSqqzEVK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinhyeuol1205/EGGROLL/blob/main/EGGROLL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RpWUFdgoH91"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import csv\n",
        "import time\n",
        "import math\n",
        "import operator\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Optional, Literal, Dict, List, Any, Tuple, Callable\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    raise ImportError(\"transformers is required. Install with: pip install transformers\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Configuration\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class EggrollTrainerConfig:\n",
        "    \"\"\"\n",
        "    Complete configuration for EGGROLL training.\n",
        "    Mirrors Args from the original JAX implementation.\n",
        "    \"\"\"\n",
        "    # Random seed\n",
        "    seed: int = 0\n",
        "\n",
        "    # Model configuration\n",
        "    model_name: str = \"Helsinki-NLP/opus-mt-en-vi\"\n",
        "\n",
        "    # Output directories\n",
        "    save_path: str = \"./checkpoints\"\n",
        "    load_path: Optional[str] = None\n",
        "\n",
        "    # Save/Load options\n",
        "    save_model: bool = True\n",
        "    load_model: bool = False\n",
        "\n",
        "    # Generation settings\n",
        "    num_beams: int = 1\n",
        "\n",
        "    # EGGROLL core hyperparameters\n",
        "    sigma: float = 1e-3              # Noise standard deviation (σ)\n",
        "    lr_scale: float = 1.0            # Learning rate (α)\n",
        "    rank: int = 16                   # Low-rank dimension (r)\n",
        "    noise_reuse: int = 1             # Reuse noise across epochs\n",
        "    freeze_nonlora: bool = True      # Freeze non-LoRA parameters\n",
        "\n",
        "    # Population settings\n",
        "    generations_per_prompt: int = 8  # N: population size per unique prompt\n",
        "    prompts_per_epoch: int = 8       # Number of unique prompts per epoch\n",
        "\n",
        "    # Training settings\n",
        "    num_epochs: int = 100\n",
        "    validate_every: int = 10\n",
        "    save_every: int = 10\n",
        "    log_every: int = 1\n",
        "    log_samples_every: int = 10\n",
        "\n",
        "    # Validation settings\n",
        "    validation_samples: int = 100\n",
        "\n",
        "    # Optimizer settings\n",
        "    optimizer_type: str = \"sgd\"      # \"sgd\" or \"adam\"\n",
        "    adam_beta1: float = 0.9\n",
        "    adam_beta2: float = 0.999\n",
        "    adam_eps: float = 1e-8\n",
        "    momentum: float = 0.0\n",
        "\n",
        "    # Reward settings\n",
        "    reward_metric: str = \"bleu\"      # \"bleu\", \"meteor\", \"chrf\", \"composite\"\n",
        "    fitness_shaping: str = \"centered_rank\"  # \"none\", \"standardize\", \"centered_rank\"\n",
        "\n",
        "    # Device settings\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "    @property\n",
        "    def total_generations_per_epoch(self) -> int:\n",
        "        return self.generations_per_prompt * self.prompts_per_epoch\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Data Structures\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TrainingStats:\n",
        "    \"\"\"Statistics for a single epoch\"\"\"\n",
        "    epoch: int\n",
        "\n",
        "    # Fitness statistics\n",
        "    avg_fitness: float = 0.0\n",
        "    std_fitness: float = 0.0\n",
        "    max_fitness: float = 0.0\n",
        "    min_fitness: float = 0.0\n",
        "    median_fitness: float = 0.0\n",
        "\n",
        "    # Update statistics\n",
        "    lora_param_diff: float = 0.0\n",
        "    full_param_diff: float = 0.0\n",
        "    gradient_norm: float = 0.0\n",
        "\n",
        "    # Timing\n",
        "    prompt_time: float = 0.0\n",
        "    generation_time: float = 0.0\n",
        "    fitness_time: float = 0.0\n",
        "    update_time: float = 0.0\n",
        "    validation_time: float = 0.0\n",
        "    saving_time: float = 0.0\n",
        "    total_time: float = 0.0\n",
        "\n",
        "    # Validation\n",
        "    validation_score: Optional[float] = None\n",
        "\n",
        "    # Running averages\n",
        "    true_train_avg_fitness: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Checkpoint:\n",
        "    \"\"\"Checkpoint data structure\"\"\"\n",
        "    epoch: int\n",
        "    params: Dict[str, torch.Tensor]\n",
        "    optimizer_state: Dict[str, Any]\n",
        "    es_map: Dict[str, int]\n",
        "    config: Dict[str, Any]\n",
        "    stats: Dict[str, float]\n",
        "    timestamp: str\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Random Key Generator (JAX-style)\n",
        "# ============================================================================\n",
        "\n",
        "class RandomKeyGenerator:\n",
        "    \"\"\"JAX-style random key generator for reproducible noise generation.\"\"\"\n",
        "\n",
        "    def __init__(self, seed: int):\n",
        "        self.seed = seed\n",
        "\n",
        "    def fold_in(self, key_id: int) -> 'RandomKeyGenerator':\n",
        "        new_seed = ((self.seed * 31337) + key_id) % (2**31)\n",
        "        return RandomKeyGenerator(new_seed)\n",
        "\n",
        "    def split(self, num_keys: int) -> List['RandomKeyGenerator']:\n",
        "        return [self.fold_in(i) for i in range(num_keys)]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# ES Map Types\n",
        "# ============================================================================\n",
        "\n",
        "class ESMapType:\n",
        "    FULL = 0\n",
        "    LORA = 1\n",
        "    FROZEN = 2\n",
        "    NOOP = 3\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Optimizer State\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class OptimizerState:\n",
        "    step: int = 0\n",
        "    momentum: Optional[Dict[str, torch.Tensor]] = None\n",
        "    velocity: Optional[Dict[str, torch.Tensor]] = None\n",
        "class EggrollTrainer:\n",
        "    \"\"\"\n",
        "    Complete EGGROLL trainer for translation model finetuning.\n",
        "\n",
        "    Implements the full training loop from the paper:\n",
        "    \"Evolution Strategies at Hyperscale\" (arXiv:2511.16652)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: EggrollTrainerConfig):\n",
        "        \"\"\"\n",
        "        Initialize the EGGROLL trainer.\n",
        "\n",
        "        Args:\n",
        "            config: Training configuration\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "\n",
        "        # Set random seeds\n",
        "        self._set_seeds(config.seed)\n",
        "\n",
        "        # Initialize components (will be set in setup())\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.params = None\n",
        "        self.es_map = None\n",
        "        self.base_evo_keys = None\n",
        "        self.opt_state = None\n",
        "        self.reward_function = None\n",
        "\n",
        "        # Training state\n",
        "        self.current_epoch = 0\n",
        "        self.true_train_fitness_sum = 0.0\n",
        "        self.best_validation_score = -float('inf')\n",
        "\n",
        "        # Timing\n",
        "        self.start_time = None\n",
        "\n",
        "\n",
        "    def _set_seeds(self, seed: int):\n",
        "        \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "        torch.manual_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Step 1: Initialization\n",
        "    # ========================================================================\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"\n",
        "        Setup the trainer (Step 1: Initialization).\n",
        "\n",
        "        This includes:\n",
        "        - Loading the pre-trained model\n",
        "        - Building ES parameter map\n",
        "        - Initializing noiser parameters\n",
        "        - Setting up reward function\n",
        "        - Initializing wandb (if enabled)\n",
        "        \"\"\"\n",
        "        print(\"=\" * 70)\n",
        "        print(\"EGGROLL Trainer Setup\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # 1. Load model and tokenizer\n",
        "        print(\"\\n[1/6] Loading model and tokenizer...\")\n",
        "        self._load_model()\n",
        "\n",
        "        # 2. Extract parameters and build ES map\n",
        "        print(\"\\n[2/6] Building ES parameter map...\")\n",
        "        self._build_es_map()\n",
        "\n",
        "        # 3. Initialize random keys\n",
        "        print(\"\\n[3/6] Initializing random keys...\")\n",
        "        self._init_random_keys()\n",
        "\n",
        "        # 4.Initialize optimizer state\n",
        "        print(\"\\n[4/6] Initializing optimizer...\")\n",
        "        self._init_optimizer()\n",
        "\n",
        "        # 5.Setup reward function\n",
        "        print(\"\\n[5/6] Setting up reward function...\")\n",
        "        self._setup_reward_function()\n",
        "\n",
        "        # 7.Load checkpoint if specified\n",
        "        if self.config.load_model and self.config.load_path:\n",
        "            print(f\"\\nLoading checkpoint from: {self.config.load_path}\")\n",
        "            self._load_checkpoint(self.config.load_path)\n",
        "\n",
        "        self._print_setup_summary()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Load pre-trained model and tokenizer.\"\"\"\n",
        "\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "            self.config.model_name,\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
        "\n",
        "        # Set model to eval mode (we don't use gradients)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Extract parameters\n",
        "        self.params = {\n",
        "            name: param.data.clone()\n",
        "            for name, param in self.model.named_parameters()\n",
        "        }\n",
        "\n",
        "        print(f\"  Model: {self.model.__class__.__name__}\")\n",
        "        print(f\"  Parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "    def _build_es_map(self):\n",
        "        \"\"\"Build ES parameter classification map.\"\"\"\n",
        "        lora_targets = [\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",\n",
        "            \"fc1\", \"fc2\",\n",
        "        ]\n",
        "\n",
        "        self.es_map = {}\n",
        "        lora_count = 0\n",
        "        full_count = 0\n",
        "        frozen_count = 0\n",
        "\n",
        "        for name, param in self.params.items():\n",
        "            # Freeze embeddings and layer norms\n",
        "            if \"embed\" in name.lower():\n",
        "                self.es_map[name] = ESMapType.FROZEN\n",
        "                frozen_count += 1\n",
        "            elif \"layer_norm\" in name.lower() or \"layernorm\" in name.lower():\n",
        "                self.es_map[name] = ESMapType.FROZEN\n",
        "                frozen_count += 1\n",
        "            # Biases get full updates (if not frozen)\n",
        "            elif \"bias\" in name.lower():\n",
        "                if self.config.freeze_nonlora:\n",
        "                    self.es_map[name] = ESMapType.FROZEN\n",
        "                    frozen_count += 1\n",
        "                else:\n",
        "                    self.es_map[name] = ESMapType.FULL\n",
        "                    full_count += 1\n",
        "            # Check for LoRA targets (2D weight matrices)\n",
        "            elif any(target in name.lower() for target in lora_targets) and len(param.shape) == 2:\n",
        "                self.es_map[name] = ESMapType.LORA\n",
        "                lora_count += 1\n",
        "            else:\n",
        "                if self.config.freeze_nonlora:\n",
        "                    self.es_map[name] = ESMapType.FROZEN\n",
        "                    frozen_count += 1\n",
        "                else:\n",
        "                    self.es_map[name] = ESMapType.FULL\n",
        "                    full_count += 1\n",
        "\n",
        "        print(f\"  LoRA parameters: {lora_count}\")\n",
        "        print(f\"  Full parameters: {full_count}\")\n",
        "        print(f\"  Frozen parameters: {frozen_count}\")\n",
        "\n",
        "    def _init_random_keys(self):\n",
        "        \"\"\"Initialize random keys for each parameter.\"\"\"\n",
        "        master_key = RandomKeyGenerator(self.config.seed)\n",
        "        self.base_model_key = master_key.fold_in(0)\n",
        "        self.base_gen_key = master_key.fold_in(1)\n",
        "        self.base_valid_key = master_key.fold_in(2)\n",
        "\n",
        "        self.base_evo_keys = {\n",
        "            name: self.base_model_key.fold_in(i)\n",
        "            for i, name in enumerate(self.params.keys())\n",
        "        }\n",
        "\n",
        "    def _init_optimizer(self):\n",
        "        \"\"\"Initialize optimizer state.\"\"\"\n",
        "        self.opt_state = OptimizerState(step=0)\n",
        "\n",
        "        if self.config.optimizer_type == \"adam\":\n",
        "            self.opt_state.momentum = {\n",
        "                name: torch.zeros_like(p)\n",
        "                for name, p in self.params.items()\n",
        "                if self.es_map.get(name, ESMapType.FROZEN) != ESMapType.FROZEN\n",
        "            }\n",
        "            self.opt_state.velocity = {\n",
        "                name: torch.zeros_like(p)\n",
        "                for name, p in self.params.items()\n",
        "                if self.es_map.get(name, ESMapType.FROZEN) != ESMapType.FROZEN\n",
        "            }\n",
        "        elif self.config.momentum > 0:\n",
        "            self.opt_state.momentum = {\n",
        "                name: torch.zeros_like(p)\n",
        "                for name, p in self.params.items()\n",
        "                if self.es_map.get(name, ESMapType.FROZEN) != ESMapType.FROZEN\n",
        "            }\n",
        "\n",
        "    def _setup_reward_function(self):\n",
        "        \"\"\"Setup reward function for evaluation.\"\"\"\n",
        "        metric = self.config.reward_metric.lower()\n",
        "\n",
        "        if metric == \"bleu\":\n",
        "            try:\n",
        "                import sacrebleu\n",
        "                self._sacrebleu = sacrebleu\n",
        "                self.reward_function = self._compute_bleu\n",
        "                print(f\"  Reward: BLEU (sacrebleu)\")\n",
        "            except ImportError:\n",
        "                self.reward_function = self._compute_bleu_nltk\n",
        "                print(f\"  Reward: BLEU (nltk)\")\n",
        "        elif metric == \"comet\":\n",
        "            try:\n",
        "                from comet import download_model, load_from_checkpoint\n",
        "                # Choose your model from Hugging Face Hub\n",
        "                # model_path = download_model(\"Unbabel/XCOMET-XL\")\n",
        "                # or for example:\n",
        "                model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "\n",
        "                # Load the model checkpoint:\n",
        "                model_comet = load_from_checkpoint(model_path)\n",
        "\n",
        "                self._comet = model_comet\n",
        "                self.reward_function = self._compute_comet\n",
        "                print(f\"  Reward: COMET (Unbabel/wmt22-comet-da)\")\n",
        "            except ImportError:\n",
        "                print(f\"COMET Error\")\n",
        "        elif metric == \"length\":\n",
        "            self.reward_function = self._compute_length_ratio\n",
        "            print(f\"  Reward: Length Ratio\")\n",
        "        else:\n",
        "            self.reward_function = self._compute_bleu_nltk\n",
        "            print(f\"  Reward: BLEU (nltk, fallback)\")\n",
        "\n",
        "    def _print_setup_summary(self):\n",
        "        \"\"\"Print setup summary.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Setup Complete!\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\"\"\n",
        "Configuration:\n",
        "  Model: {self.config.model_name}\n",
        "  Device: {self.device}\n",
        "\n",
        "EGGROLL Hyperparameters:\n",
        "  σ (sigma): {self.config.sigma}\n",
        "  α (learning rate): {self.config.lr_scale}\n",
        "  r (rank): {self.config.rank}\n",
        "  N (population per prompt): {self.config.generations_per_prompt}\n",
        "\n",
        "Training:\n",
        "  Epochs: {self.config.num_epochs}\n",
        "  Prompts per epoch: {self.config.prompts_per_epoch}\n",
        "  Total generations per epoch: {self.config.total_generations_per_epoch}\n",
        "  Reward metric: {self.config.reward_metric}\n",
        "\"\"\")\n",
        "    # ========================================================================\n",
        "    # Step 2 & 3: Perturbation and Forward Pass\n",
        "    # ========================================================================\n",
        "\n",
        "    def _get_perturbation_seed(self, base_seed: int, epoch: int, member_idx: int) -> int:\n",
        "        \"\"\"Get deterministic seed for perturbation.\"\"\"\n",
        "        if self.config.noise_reuse > 0:\n",
        "            effective_epoch = epoch // self.config.noise_reuse\n",
        "        else:\n",
        "            effective_epoch = epoch\n",
        "        return ((base_seed * 31337) + effective_epoch * 1000 + member_idx) % (2**31)\n",
        "\n",
        "    def _generate_lora_perturbation(\n",
        "        self,\n",
        "        param_shape: Tuple[int, int],\n",
        "        seed: int,\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Generate low-rank perturbation matrices A and B.\"\"\"\n",
        "        out_features, in_features = param_shape\n",
        "        rank = self.config.rank\n",
        "        sigma = self.config.sigma\n",
        "\n",
        "        gen_A = torch.Generator().manual_seed(seed)\n",
        "        gen_B = torch.Generator().manual_seed(seed + 1)\n",
        "\n",
        "        # Scale by σ/√r\n",
        "        scale = sigma / math.sqrt(rank)\n",
        "\n",
        "        A = torch.randn(out_features, rank, generator=gen_A) * scale\n",
        "        B = torch.randn(in_features, rank, generator=gen_B)\n",
        "\n",
        "        return A.to(self.device), B.to(self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_with_perturbation(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        epoch: int,\n",
        "        member_idx: int,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate translation with perturbed model (Steps 2 & 3).\n",
        "\n",
        "        Applies perturbation by directly modifying weights temporarily.\n",
        "        \"\"\"\n",
        "        # Store original weights\n",
        "        original_weights = {}\n",
        "\n",
        "        # Apply perturbations\n",
        "        for name, param in self.model.named_parameters():\n",
        "            map_type = self.es_map.get(name, ESMapType.FROZEN)\n",
        "\n",
        "            if map_type == ESMapType.FROZEN:\n",
        "                continue\n",
        "\n",
        "            original_weights[name] = param.data.clone()\n",
        "            base_seed = self.base_evo_keys[name].seed\n",
        "            seed = self._get_perturbation_seed(base_seed, epoch, member_idx)\n",
        "\n",
        "            if map_type == ESMapType.LORA and len(param.shape) == 2:\n",
        "                A, B = self._generate_lora_perturbation(param.shape, seed)\n",
        "                param.data = param.data + A @ B.T\n",
        "            elif map_type == ESMapType.FULL:\n",
        "                gen = torch.Generator().manual_seed(seed)\n",
        "                noise = torch.randn_like(param, generator=gen) * self.config.sigma\n",
        "                param.data = param.data + noise.to(self.device)\n",
        "\n",
        "        # Generate\n",
        "        output_ids = self.model.generate(\n",
        "            input_ids=input_ids,\n",
        "            num_beams=self.config.num_beams,\n",
        "        )\n",
        "\n",
        "        # Restore original weights\n",
        "        for name, original in original_weights.items():\n",
        "            param = dict(self.model.named_parameters())[name]\n",
        "            param.data = original\n",
        "\n",
        "        return output_ids\n",
        "\n",
        "    # ========================================================================\n",
        "    # Step 4: Reward Computation\n",
        "    # ========================================================================\n",
        "\n",
        "    def _compute_bleu(self, hypothesis: str, reference: str) -> float:\n",
        "        \"\"\"Compute BLEU score using sacrebleu.\"\"\"\n",
        "        if not hypothesis.strip() or not reference.strip():\n",
        "            return 0.0\n",
        "        try:\n",
        "            bleu = self._sacrebleu.sentence_bleu(hypothesis, [reference], smooth_method='exp')\n",
        "            return bleu.score / 100.0\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _compute_comet(self, sources: List[str], hypothesis: List[str], reference: List[str]) -> np.ndarray:\n",
        "        # Data must be in the following format:\n",
        "        data = [\n",
        "            {\n",
        "                \"src\": src,\n",
        "                \"mt\": hyp,\n",
        "                \"ref\": ref\n",
        "            } for src, hyp, ref in zip(sources, hypothesis, reference)\n",
        "        ]\n",
        "\n",
        "        if not sources or not hypothesis or not reference:\n",
        "            return np.zeros(len(sources))\n",
        "        try:\n",
        "            model_output = self._comet.predict(data, batch_size=8, gpus=1)\n",
        "            return np.array(model_output.scores)\n",
        "        except:\n",
        "            return np.zeros(len(sources))\n",
        "\n",
        "    def _compute_bleu_nltk(self, hypothesis: str, reference: str) -> float:\n",
        "        \"\"\"Compute BLEU score using nltk.\"\"\"\n",
        "        if not hypothesis.strip() or not reference.strip():\n",
        "            return 0.0\n",
        "        try:\n",
        "            from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "            hyp_tokens = hypothesis.lower().split()\n",
        "            ref_tokens = reference.lower().split()\n",
        "            if len(hyp_tokens) == 0:\n",
        "                return 0.0\n",
        "            return sentence_bleu([ref_tokens], hyp_tokens,\n",
        "                               smoothing_function=SmoothingFunction().method1)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def _compute_length_ratio(self, hypothesis: str, reference: str) -> float:\n",
        "        \"\"\"Compute length ratio reward.\"\"\"\n",
        "        if not reference.strip():\n",
        "            return 0.0\n",
        "        hyp_len = len(hypothesis.split())\n",
        "        ref_len = len(reference.split())\n",
        "        if ref_len == 0:\n",
        "            return 0.0\n",
        "        ratio = hyp_len / ref_len\n",
        "        return max(0.0, 1.0 - abs(ratio - 1.0))\n",
        "\n",
        "    def _compute_rewards(\n",
        "        self,\n",
        "        sources: List[str],\n",
        "        hypotheses: List[str],\n",
        "        references: List[str],\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Compute rewards for all hypotheses.\"\"\"\n",
        "        metric = self.config.reward_metric.lower()\n",
        "        if metric == \"comet\":\n",
        "            rewards = self.reward_function(sources, hypotheses, references)\n",
        "        else:\n",
        "            rewards = np.array([\n",
        "                self.reward_function(hyp, ref)\n",
        "                for hyp, ref in zip(hypotheses, references)\n",
        "            ])\n",
        "        return rewards\n",
        "\n",
        "    # ========================================================================\n",
        "    # Step 5: Fitness Shaping\n",
        "    # ========================================================================\n",
        "\n",
        "    def _shape_fitnesses(self, raw_scores: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply fitness shaping for ES stability.\n",
        "\n",
        "        Mirrors NOISER.convert_fitnesses from original code.\n",
        "        \"\"\"\n",
        "        if self.config.fitness_shaping == \"none\":\n",
        "            return raw_scores\n",
        "\n",
        "        elif self.config.fitness_shaping == \"standardize\":\n",
        "            mean = np.mean(raw_scores)\n",
        "            std = np.std(raw_scores) + 1e-8\n",
        "            return (raw_scores - mean) / std\n",
        "\n",
        "        elif self.config.fitness_shaping == \"centered_rank\":\n",
        "            n = len(raw_scores)\n",
        "            ranks = np.argsort(np.argsort(raw_scores))\n",
        "            shaped = (ranks.astype(np.float32) + 0.5) / n - 0.5\n",
        "            return shaped\n",
        "\n",
        "        else:\n",
        "            # Group-wise normalization (for multiple prompts)\n",
        "            group_size = self.config.generations_per_prompt\n",
        "            if group_size > 0 and len(raw_scores) > group_size:\n",
        "                group_scores = raw_scores.reshape(-1, group_size)\n",
        "                group_mean = np.mean(group_scores, axis=-1, keepdims=True)\n",
        "                global_std = np.std(raw_scores) + 1e-8\n",
        "                shaped = (group_scores - group_mean) / global_std\n",
        "                return shaped.ravel()\n",
        "            else:\n",
        "                mean = np.mean(raw_scores)\n",
        "                std = np.std(raw_scores) + 1e-8\n",
        "                return (raw_scores - mean) / std\n",
        "\n",
        "    # ========================================================================\n",
        "    # Steps 5 & 6: Gradient Estimation and Update\n",
        "    # ========================================================================\n",
        "\n",
        "    def _estimate_and_update(\n",
        "        self,\n",
        "        shaped_fitnesses: np.ndarray,\n",
        "        epoch: int,\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Estimate gradients and update parameters (Steps 5 & 6).\n",
        "\n",
        "        Mirrors _do_update from original code.\n",
        "        \"\"\"\n",
        "        population_size = len(shaped_fitnesses)\n",
        "        stats = {}\n",
        "\n",
        "        new_params = {}\n",
        "        lora_diff_sum = 0.0\n",
        "        lora_count = 0\n",
        "        full_diff_sum = 0.0\n",
        "        full_count = 0\n",
        "        total_grad_norm_sq = 0.0\n",
        "\n",
        "        for name, param in self.params.items():\n",
        "            map_type = self.es_map.get(name, ESMapType.FROZEN)\n",
        "\n",
        "            if map_type == ESMapType.FROZEN:\n",
        "                new_params[name] = param\n",
        "                continue\n",
        "\n",
        "            # Estimate gradient\n",
        "            gradient = torch.zeros_like(param)\n",
        "\n",
        "            for member_idx in range(population_size):\n",
        "                R_i = shaped_fitnesses[member_idx]\n",
        "                base_seed = self.base_evo_keys[name].seed\n",
        "                seed = self._get_perturbation_seed(base_seed, epoch, member_idx)\n",
        "\n",
        "                if map_type == ESMapType.LORA and len(param.shape) == 2:\n",
        "                    A, B = self._generate_lora_perturbation(param.shape, seed)\n",
        "                    gradient += R_i * (A @ B.T)\n",
        "                elif map_type == ESMapType.FULL:\n",
        "                    gen = torch.Generator().manual_seed(seed)\n",
        "                    noise = torch.randn_like(param, generator=gen) * self.config.sigma\n",
        "                    gradient += R_i * noise.to(self.device) / self.config.sigma\n",
        "\n",
        "            gradient /= population_size\n",
        "\n",
        "            # Apply optimizer\n",
        "            update = self._apply_optimizer_step(name, gradient)\n",
        "\n",
        "            # Update parameter (ADD because ES maximizes reward)\n",
        "            new_param = param + update\n",
        "            new_params[name] = new_param\n",
        "\n",
        "            # Compute difference\n",
        "            diff = torch.sqrt(torch.mean((new_param - param) ** 2)).item()\n",
        "\n",
        "            if map_type == ESMapType.LORA:\n",
        "                lora_diff_sum += diff\n",
        "                lora_count += 1\n",
        "            else:\n",
        "                full_diff_sum += diff\n",
        "                full_count += 1\n",
        "\n",
        "            total_grad_norm_sq += torch.norm(gradient).item() ** 2\n",
        "\n",
        "        # Update stored parameters\n",
        "        self.params = new_params\n",
        "\n",
        "        # Update model weights\n",
        "        self._update_model_weights()\n",
        "\n",
        "        stats['gradient_norm'] = math.sqrt(total_grad_norm_sq)\n",
        "        stats['lora_param_diff'] = lora_diff_sum / max(lora_count, 1)\n",
        "        stats['full_param_diff'] = full_diff_sum / max(full_count, 1)\n",
        "\n",
        "        return stats\n",
        "    def _apply_optimizer_step(\n",
        "        self,\n",
        "        name: str,\n",
        "        gradient: torch.Tensor,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Apply optimizer step to gradient.\"\"\"\n",
        "        lr = self.config.lr_scale\n",
        "\n",
        "        if self.config.optimizer_type == \"adam\":\n",
        "            t = self.opt_state.step + 1\n",
        "            beta1 = self.config.adam_beta1\n",
        "            beta2 = self.config.adam_beta2\n",
        "            eps = self.config.adam_eps\n",
        "\n",
        "            m = self.opt_state.momentum.get(name, torch.zeros_like(gradient))\n",
        "            v = self.opt_state.velocity.get(name, torch.zeros_like(gradient))\n",
        "\n",
        "            m = beta1 * m + (1 - beta1) * gradient\n",
        "            v = beta2 * v + (1 - beta2) * (gradient ** 2)\n",
        "\n",
        "            self.opt_state.momentum[name] = m\n",
        "            self.opt_state.velocity[name] = v\n",
        "\n",
        "            m_hat = m / (1 - beta1 ** t)\n",
        "            v_hat = v / (1 - beta2 ** t)\n",
        "\n",
        "            return lr * m_hat / (torch.sqrt(v_hat) + eps)\n",
        "\n",
        "        elif self.config.momentum > 0:\n",
        "            m = self.opt_state.momentum.get(name, torch.zeros_like(gradient))\n",
        "            m = self.config.momentum * m + gradient\n",
        "            self.opt_state.momentum[name] = m\n",
        "            return lr * m\n",
        "\n",
        "        else:\n",
        "            return lr * gradient\n",
        "\n",
        "    def _update_model_weights(self):\n",
        "        \"\"\"Update model weights from params dictionary.\"\"\"\n",
        "        state_dict = self.model.state_dict()\n",
        "        for name, param in self.params.items():\n",
        "            if name in state_dict:\n",
        "                state_dict[name] = param\n",
        "        self.model.load_state_dict(state_dict)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Single Epoch\n",
        "    # ========================================================================\n",
        "\n",
        "    def _single_epoch(\n",
        "        self,\n",
        "        train_data: List[Tuple[str, str]],\n",
        "        epoch: int,\n",
        "        val_data: Optional[List[Tuple[str, str]]] = None,\n",
        "    ) -> TrainingStats:\n",
        "        \"\"\"\n",
        "        Execute a single training epoch.\n",
        "\n",
        "        Mirrors single_epoch from original code.\n",
        "        \"\"\"\n",
        "        stats = TrainingStats(epoch=epoch)\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Validation (periodic)\n",
        "        if epoch % self.config.validate_every == 0 and val_data:\n",
        "            val_start = time.time()\n",
        "            stats.validation_score = self._validate(val_data)\n",
        "            stats.validation_time = time.time() - val_start\n",
        "\n",
        "            if stats.validation_score > self.best_validation_score:\n",
        "                self.best_validation_score = stats.validation_score\n",
        "\n",
        "        # Sample prompts for this epoch\n",
        "        prompt_start = time.time()\n",
        "        epoch_samples = self._sample_epoch_data(train_data, epoch)\n",
        "        stats.prompt_time = time.time() - prompt_start\n",
        "\n",
        "        # Generate with all population members\n",
        "        gen_start = time.time()\n",
        "\n",
        "        all_references = [reference for _, reference in epoch_samples]\n",
        "        all_sources = [source for source, _ in epoch_samples]\n",
        "\n",
        "        print(\"Batch tokenizing...\")\n",
        "        batch_inputs = self.tokenizer(\n",
        "            all_sources,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        ).to(self.device)\n",
        "\n",
        "        # for source, reference in epoch_samples:\n",
        "        #     # Tokenize\n",
        "        #     inputs = self.tokenizer(\n",
        "        #         source,\n",
        "        #         return_tensors=\"pt\",\n",
        "        #         padding=True,\n",
        "        #     ).to(self.device)\n",
        "\n",
        "        # Generate for each population member\n",
        "        batch_hypotheses = []\n",
        "        batch_references = []\n",
        "        batch_sources = []\n",
        "\n",
        "        for member_idx in range(self.config.generations_per_prompt):\n",
        "            batch_output_ids = self._generate_with_perturbation(\n",
        "                batch_inputs[\"input_ids\"],\n",
        "                epoch,\n",
        "                member_idx,\n",
        "            )\n",
        "\n",
        "            # hypothesis = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "            hypothesis = [\n",
        "                self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "                for output_ids in batch_output_ids\n",
        "            ]\n",
        "            batch_hypotheses.extend(hypothesis)\n",
        "            batch_references.extend(all_references)\n",
        "            batch_sources.extend(all_sources)\n",
        "\n",
        "        stats.generation_time = time.time() - gen_start\n",
        "\n",
        "        # Compute rewards (Step 4)\n",
        "        fitness_start = time.time()\n",
        "        raw_rewards = self._compute_rewards(batch_sources, batch_hypotheses, batch_references)\n",
        "\n",
        "        # Aggregate rewards per direction\n",
        "        raw_rewards = raw_rewards.reshape(\n",
        "            self.config.prompts_per_epoch,\n",
        "            self.config.generations_per_prompt\n",
        "        ).sum(axis=0)\n",
        "\n",
        "        stats.fitness_time = time.time() - fitness_start\n",
        "        # Statistics\n",
        "        stats.avg_fitness = float(np.mean(raw_rewards))\n",
        "        stats.std_fitness = float(np.std(raw_rewards))\n",
        "        stats.max_fitness = float(np.max(raw_rewards))\n",
        "        stats.min_fitness = float(np.min(raw_rewards))\n",
        "        stats.median_fitness = float(np.median(raw_rewards))\n",
        "\n",
        "        # Shape fitnesses (Step 5a)\n",
        "        shaped_fitnesses = self._shape_fitnesses(raw_rewards)\n",
        "\n",
        "        # Estimate gradients and update (Steps 5b & 6)\n",
        "        update_start = time.time()\n",
        "        update_stats = self._estimate_and_update(shaped_fitnesses, epoch)\n",
        "        stats.update_time = time.time() - update_start\n",
        "\n",
        "        stats.lora_param_diff = update_stats['lora_param_diff']\n",
        "        stats.full_param_diff = update_stats['full_param_diff']\n",
        "        stats.gradient_norm = update_stats['gradient_norm']\n",
        "\n",
        "        # Increment optimizer step\n",
        "        self.opt_state.step += 1\n",
        "\n",
        "        # Update running average\n",
        "        self.true_train_fitness_sum += np.sum(raw_rewards)\n",
        "        stats.true_train_avg_fitness = (\n",
        "            self.true_train_fitness_sum /\n",
        "            ((epoch + 1) * self.config.generations_per_prompt)\n",
        "        )\n",
        "\n",
        "        # Save checkpoint (periodic)\n",
        "        if self.config.save_model and epoch % self.config.save_every == 0:\n",
        "            save_start = time.time()\n",
        "            self._save_checkpoint(epoch, stats)\n",
        "            stats.saving_time = time.time() - save_start\n",
        "\n",
        "        stats.total_time = time.time() - epoch_start\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def _sample_epoch_data(\n",
        "        self,\n",
        "        train_data: List[Tuple[str, str]],\n",
        "        epoch: int,\n",
        "    ) -> List[Tuple[str, str]]:\n",
        "        \"\"\"Sample data for this epoch.\"\"\"\n",
        "        # Use epoch as seed for reproducible sampling\n",
        "        rng = np.random.RandomState(self.config.seed + epoch)\n",
        "        indices = rng.choice(\n",
        "            len(train_data),\n",
        "            size=min(self.config.prompts_per_epoch, len(train_data)),\n",
        "            replace=False,\n",
        "        )\n",
        "        return [train_data[i] for i in indices]\n",
        "\n",
        "    # ========================================================================\n",
        "    # Validation\n",
        "    # ========================================================================\n",
        "\n",
        "    @torch.no_grad()\n",
        "    # def _validate(self, epoch: int) -> float:\n",
        "    #     \"\"\"Run validation without perturbation (σ=0).\"\"\"\n",
        "    #     # This would use validation data\n",
        "    #     # For now, return placeholder\n",
        "    #     return 0.0\n",
        "\n",
        "    def _validate(\n",
        "        self,\n",
        "        val_data: List[Tuple[str, str]],\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Run validation on provided data.\n",
        "\n",
        "        Uses base model without perturbation.\n",
        "        \"\"\"\n",
        "\n",
        "        # Unpack the tuples into two separate lists\n",
        "        list1, list2 = zip(*val_data)\n",
        "\n",
        "        # Convert the zip objects to lists\n",
        "        sources, references = list(list1), list(list2)\n",
        "\n",
        "        batch_inputs = self.tokenizer(\n",
        "            sources,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            batch_output_ids = self.model.generate(\n",
        "                input_ids=batch_inputs[\"input_ids\"],\n",
        "                num_beams=self.config.num_beams,\n",
        "            )\n",
        "        hypothesis = [\n",
        "            self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "            for output_ids in batch_output_ids\n",
        "        ]\n",
        "\n",
        "        reward = self.reward_function(sources, hypothesis, references)\n",
        "\n",
        "        # total_reward = 0.0\n",
        "        # count = 0\n",
        "\n",
        "        # for source, reference in tqdm(val_data, desc=\"Validating\"):\n",
        "        #     inputs = self.tokenizer(\n",
        "        #         source,\n",
        "        #         return_tensors=\"pt\",\n",
        "        #         padding=True,\n",
        "        #         truncation=True,\n",
        "        #     ).to(self.device)\n",
        "\n",
        "        #     with torch.no_grad():\n",
        "        #         output_ids = self.model.generate(\n",
        "        #             input_ids=inputs[\"input_ids\"],\n",
        "        #             num_beams=self.config.num_beams,\n",
        "        #         )\n",
        "\n",
        "        #     hypothesis = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "        #     reward = self.reward_function(source, hypothesis, reference)\n",
        "\n",
        "        #     total_reward += reward\n",
        "        #     count += 1\n",
        "\n",
        "        # return total_reward / max(count, 1)\n",
        "        return reward.mean()\n",
        "\n",
        "    # ========================================================================\n",
        "    # Checkpointing\n",
        "    # ========================================================================\n",
        "\n",
        "    def _save_checkpoint(self, epoch: int, stats: TrainingStats):\n",
        "        \"\"\"Save training checkpoint.\"\"\"\n",
        "        ckpt_dir = Path(self.config.save_path)\n",
        "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'params': {k: v.cpu() for k, v in self.params.items()},\n",
        "            'opt_state': {\n",
        "                'step': self.opt_state.step,\n",
        "                'momentum': {k: v.cpu() for k, v in (self.opt_state.momentum or {}).items()},\n",
        "                'velocity': {k: v.cpu() for k, v in (self.opt_state.velocity or {}).items()},\n",
        "            },\n",
        "            'es_map': self.es_map,\n",
        "            'config': asdict(self.config),\n",
        "            'stats': asdict(stats),\n",
        "            'true_train_fitness_sum': self.true_train_fitness_sum,\n",
        "            'best_validation_score': self.best_validation_score,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "        }\n",
        "\n",
        "        ckpt_path = ckpt_dir / f\"checkpoint_epoch_{epoch:05d}.pt\"\n",
        "        torch.save(checkpoint, ckpt_path)\n",
        "\n",
        "        # Also save as latest\n",
        "        latest_path = ckpt_dir / \"latest.pt\"\n",
        "        torch.save(checkpoint, latest_path)\n",
        "\n",
        "        self.model.save_pretrained(f\"{ckpt_dir}/checkpoint_epoch_{epoch:05d}\")\n",
        "        self.tokenizer.save_pretrained(f\"{ckpt_dir}/checkpoint_epoch_{epoch:05d}\")\n",
        "\n",
        "        self.model.save_pretrained(f\"{ckpt_dir}/checkpoint_last\")\n",
        "        self.tokenizer.save_pretrained(f\"{ckpt_dir}/checkpoint_last\")\n",
        "\n",
        "        print(f\"  Checkpoint saved: {ckpt_path}\")\n",
        "\n",
        "    def _load_checkpoint(self, path: str):\n",
        "        \"\"\"Load training checkpoint.\"\"\"\n",
        "        checkpoint = torch.load(path, map_location=self.device)\n",
        "\n",
        "        self.params = {\n",
        "            k: v.to(self.device)\n",
        "            for k, v in checkpoint['params'].items()\n",
        "        }\n",
        "\n",
        "        self.opt_state.step = checkpoint['opt_state']['step']\n",
        "        if checkpoint['opt_state']['momentum']:\n",
        "            self.opt_state.momentum = {\n",
        "                k: v.to(self.device)\n",
        "                for k, v in checkpoint['opt_state']['momentum'].items()\n",
        "            }\n",
        "        if checkpoint['opt_state']['velocity']:\n",
        "            self.opt_state.velocity = {\n",
        "                k: v.to(self.device)\n",
        "                for k, v in checkpoint['opt_state']['velocity'].items()\n",
        "            }\n",
        "\n",
        "        self.current_epoch = checkpoint['epoch'] + 1\n",
        "        self.true_train_fitness_sum = checkpoint.get('true_train_fitness_sum', 0.0)\n",
        "        self.best_validation_score = checkpoint.get('best_validation_score', -float('inf'))\n",
        "\n",
        "        self._update_model_weights()\n",
        "\n",
        "        print(f\"  Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Logging\n",
        "    # ========================================================================\n",
        "\n",
        "    def _log_epoch(self, stats: TrainingStats):\n",
        "        \"\"\"Log epoch statistics.\"\"\"\n",
        "        # Console logging\n",
        "        if stats.epoch % self.config.log_every == 0:\n",
        "            print(f\"\\nEpoch {stats.epoch:5d} | \"\n",
        "                  f\"Fitness: {stats.avg_fitness:.4f} ± {stats.std_fitness:.4f} | \"\n",
        "                  f\"Best: {stats.max_fitness:.4f} | \"\n",
        "                  f\"Grad: {stats.gradient_norm:.6f} | \"\n",
        "                  f\"Time: {stats.total_time:.2f}s\")\n",
        "\n",
        "            if stats.validation_score is not None:\n",
        "                print(f\"           | Validation: {stats.validation_score:.4f} \"\n",
        "                      f\"(Best: {self.best_validation_score:.4f})\")\n",
        "    def train(\n",
        "        self,\n",
        "        train_data: List[Tuple[str, str]],\n",
        "        val_data: Optional[List[Tuple[str, str]]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Main training loop.\n",
        "\n",
        "        Args:\n",
        "            train_data: List of (source, target) pairs\n",
        "            val_data: Optional validation data\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"Starting EGGROLL Training\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        self.start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            for epoch in tqdm(range(self.current_epoch, self.config.num_epochs),\n",
        "                            desc=\"Training\", initial=self.current_epoch,\n",
        "                            total=self.config.num_epochs):\n",
        "\n",
        "                # Run single epoch\n",
        "                if val_data:\n",
        "                    stats = self._single_epoch(train_data, epoch, val_data)\n",
        "                else:\n",
        "                    stats = self._single_epoch(train_data, epoch)\n",
        "\n",
        "                # Log\n",
        "                self._log_epoch(stats)\n",
        "\n",
        "                self.current_epoch = epoch + 1\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nTraining interrupted by user.\")\n",
        "\n",
        "        finally:\n",
        "            # Final save\n",
        "            if self.config.save_model:\n",
        "                print(\"\\nSaving final checkpoint...\")\n",
        "                final_stats = TrainingStats(epoch=self.current_epoch - 1)\n",
        "                self._save_checkpoint(self.current_epoch - 1, final_stats)\n",
        "\n",
        "\n",
        "        total_time = time.time() - self.start_time\n",
        "        print(f\"\\nTraining completed in {total_time/3600:.2f} hours\")\n",
        "        print(f\"Best validation score: {self.best_validation_score:.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Main Entry Point\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main entry point for EGGROLL training.\"\"\"\n",
        "\n",
        "    # Example configuration\n",
        "    config = EggrollTrainerConfig(\n",
        "        # Model\n",
        "        model_name=\"/home/jovyan/nmt-srv-shared/users/binh/grpo_training/transflow/0_Base/en-vi-2.1.10.04-grpo-100k\",\n",
        "\n",
        "        # EGGROLL hyperparameters\n",
        "        sigma=1e-3,\n",
        "        lr_scale=1.0,\n",
        "        rank=16,\n",
        "\n",
        "        # Population\n",
        "        generations_per_prompt=256,\n",
        "        prompts_per_epoch=32,\n",
        "\n",
        "        # Training\n",
        "        num_epochs=100,\n",
        "        validate_every=20,\n",
        "        save_every=10,\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer_type=\"sgd\",\n",
        "\n",
        "        # Reward\n",
        "        reward_metric=\"comet\",\n",
        "        fitness_shaping=\"centered_rank\",\n",
        "\n",
        "        # Paths\n",
        "        save_path=\"/home/jovyan/nmt-srv-shared/users/binh/EGGROLL/checkpoints\",\n",
        "\n",
        "        # Device\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    )\n",
        "\n",
        "    # Example training data (source, target pairs)\n",
        "    src_train = open(\"/home/jovyan/nmt-srv-shared/users/binh/eggroll_training/dataset/train.src\", \"r\", encoding='utf-8').readlines()\n",
        "    tgt_train = open(\"/home/jovyan/nmt-srv-shared/users/binh/eggroll_training/dataset/train.tgt\", \"r\", encoding='utf-8').readlines()\n",
        "\n",
        "    src_valid = open(\"/home/jovyan/nmt-srv-shared/users/binh/eggroll_training/dataset/valid.src\", \"r\", encoding='utf-8').readlines()\n",
        "    tgt_valid = open(\"/home/jovyan/nmt-srv-shared/users/binh/eggroll_training/dataset/valid.tgt\", \"r\", encoding='utf-8').readlines()\n",
        "\n",
        "    train_data = []\n",
        "    valid_data = []\n",
        "    for src, tgt in tqdm(zip(src_train, tgt_train), desc=\"Loading train dataset\"):\n",
        "        train_data.append((src.strip(), tgt.strip()))\n",
        "    for src, tgt in tqdm(zip(src_valid, tgt_valid), desc=\"Loading valid dataset\"):\n",
        "        valid_data.append((src.strip(), tgt.strip()))\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = EggrollTrainer(config)\n",
        "\n",
        "    # Setup\n",
        "    trainer.setup()\n",
        "\n",
        "    # Train\n",
        "    trainer.train(train_data, valid_data)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}